% vim: set textwidth=72 :
% Copyright (C) 2013 Roy E Lowrance
% See the file COPYING.txt for copying conditions.

\documentclass{article}
\usepackage{amssymb,latexsym,amsmath}
\usepackage{verbatim}
\let\code\texttt % in line source code

\begin{document}
\title{Title}
\author{Roy E Lowrance}
\maketitle

%\abstract{XXX}

\section{Problem}

You want to test your map-reduce job before you submit it to Hadoop, perhaps because the 
debugging tools in the Hadoop streaming interface are lacking.

\section{Solution}

The solution has two parts:
\begin{itemize}
  \item Write the mapper so that it can process less than all the input.
  \item In the mapper, write debugging information into the key-value
    file by using  keys designated for that purpose. Modify the reducer to
    handle these keys appropriately.
\end{itemize}

\subsection{map a subset of the input}

Write your map program so that it can run from the command line and as a
Hadoop command. When it runs from the command line, you may need to
limit the amount of input it reads, if it reads a lot of input or
creates a lot of key-value pairs for your reduce job.

To do this, write one map program that has an optional parameter. If the
optional parameter is present, its value is used to limit the scope of
the map program. One way to do this is to have let the optional
parameter be READLIMIT, an integer with a default value of -1. If
READLIMIT is non-negative, the map program stops after reading the
indicated number of input records. If READLIMIT is negative, the map
program reads and processes all the input records. (The READLIMIT idea
is common in R for controlling how many records are read and processed
from a file.)

\subsection{put debugging information into the key-value pairs}

The reducer will read all the key-value pairs after they have been
combined and sorted by key. Designate one set of keys as only containing
debugging information and have the reducer handle records with these
keys specially. The keys and values are strings. One choice is to say
that any key beginning with a ``\#'' is for debugging work, not regular
processing.


\section{Discussion}

The sample code is a modification of the code used for the recipe
``Using Torch on Hadoop.''

The mapper program is modified to accept the optional READLIMIT
argument and to write diagnostic information to its output stream using
keys that begin with ``\#''.

\verbatiminput{test-hadoop-job-assets/countInput-map.lua}

The reducer program looks for the diagnostic records and writes them to
stderr. The output is written to stdout. In some uses, you may want to
write the diagnostic output to stdout as well.

\verbatiminput{test-hadoop-job-assets/countInput-reduce.lua}

There are two shell scripts to run the tests. \code{go-map.sh} runs just
the mapper. It executes the mapper program as a command.

\verbatiminput{test-hadoop-job-assets/go-map.sh}

The shell script \code{go-map-reduce.sh} runs the mapper and the
reducer. The output of the mapper is piped to a sort command that sorts
from the first column (\code{-k 1}) to the first white space (the tab
character separating the key and value), and the pipes the sorted
key-value pairs (possibly including the diagnostic information) to the
reduce. The stdout from the reducer is saved and its stderr is printed.

\verbatiminput{test-hadoop-job-assets/go-map-reduce.sh}

Below is the input file \code{courant-abel-prize-winners.txt}.

\verbatiminput{test-hadoop-job-assets/courant-abel-prize-winners.txt}

\section{See also}

This recipe is free documentation. You can modify it by visiting
github for account ``rlowrance'' and forking the repo
``torch-cookbook.''

\end{document}


