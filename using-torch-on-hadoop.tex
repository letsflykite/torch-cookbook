% vim: set textwidth=72 :
% Copyright (C) 2013 Roy E Lowrance
% See the file COPYING.txt for copying conditions.

\documentclass{article}
\usepackage{amssymb,latexsym,amsmath}
\usepackage{listings}
\usepackage{verbatim}
\let\code\texttt % in line source code

\begin{document}
\title{Using Torch on Hadoop}
\author{Roy E Lowrance}
\maketitle

%\abstract{XXX}

\section{Problem}

You want to run a streaming job on NYU's Hadoop cluster using torch as
the programming language.

\section{Solution}

The solution is straight forward but requires getting a lot of details
rights.
\begin{enumerate}
  \item Get your data and torch scripts onto babar, the Hadoop cluster.
  \item Make sure a suitable instance of torch is available on babar,
    the Hadoop cluster.
  \item Break your map-reduce job into two torch programs. One will be
    the map command, the other the reduce command. A \emph{command} is a
    program that receives no command line options. 
  \item Use the streaming interface to submit a map-reduce job. The job
    will read a single file in the Hadoop file system. It will create a
    directory of files in the Hadoop file system.
  \item Copy the output directory from the Hadoop file system to the
    local file system for ease of examination and subsequent use.
\end{enumerate}

\section{Discussion}

We provide a worked example of a streaming job written in torch. The
code is short enough so that you can just type it in. It is also
available in the repository that holds this TeX code that generated this
document. How to get the source of this document is discussed in the
section ``See Also.''

\subsection{Move data and code to babar}

Before working through the example, there are some preliminary steps.
You need an NYU Hadoop logon. To get one, send a request in an email to
Yann LeCun. You need to install torch in your home directory 
on the Hadoop cluster. We provide a recipe for torch
installation in this cookbook.

Log in the the Hadoop cluster. In the example below, ID is your user
id. babar is the name of the login node for the Hadoop cluster.
(There is also \code{dumbo.es.its.nyu.edu}, but it doesn't have as much
software installed on it.)

\begin{verbatim}
$ ssh ID@hpc.nyu.edu
$ ssh babar.es.its.nyu.edu
\end{verbatim}


Now you should create a test file in the local file system. We provide
the file \code{courant-abel-prize-winners} which lists faculty of the
Courant Institute that won the Abel Prize. Each row contains the year of
the award, a tab character, and the name of the winner. Here's the file:

\verbatiminput{code/using-torch-on-hadoop/courant-able-prize-winners}

If you are using your own data and code, I recommend moving the data to
babar using \code{rsync} and your code to babar using \code{github}.

\subsection{Making a torch executable available}

You need an accessible instance of torch. For this you can either
install your own torch locally on babar in your user account or use
someone else's instance. I like to install my own so that I know the
torch I'm using on babar is exactly the same as the one on my
development system. To install torch locally, follow the recipe for
installing torch. At the time this note was written, Xiang
Zhang's torch install was available at \code{/home/xz558/.usr/bin}. Note
the dot before the \code{bin} directory. Xiang's torch install was up to
date when this note was written.

Be careful of the installed torch executables already on babar. At one time,
\code{dumbo}, an alternative Hadoop login node, had an installed version
of torch, but it was old and buggy.

To find out which torch executable you have, enter \code{\$which torch} in
a terminal. If you have installed torch and the \code{which} command
doesn't bring it up, fix your \code{\$PATH} variable by executing 
\code{EXPORT \$PATH:<directory with your torch>:\$PATH} in your
\code{.bash\_profile} script. If you edit your \code{.bash\_profile} you
will need to get bash to read it again. You do that by executing this
command in a terminal: \code{\$ source .bash\_profile}.

\subsection{Create the map-reduce commands}

Create two commands that will form your map-reduce job. Each command is
a torch source file that starts with a shebang. The source file is
invoked by the Hadoop run-time with no parameters, so it needs to be
self contained.

The map command reads stdin, which will be a file in the Hadoop file
system. It writes to stdout a series of records. Each record is
comprised of a key and a value separted by a tab character. Hadoop 
may run multiple instances of your map command. In fact, you hope that
it does, because that's where the speed up comes from. Each instance 
will produce a separate stdout key-value file.

Since you are using the streaming interface, you cannot specify a
combiner. 

When all the mappers finish running, their output files are split among
reducer nodes such that each reduce receives a file that has been sorted
by keys. Only one reducer is run for each key. The file a reducer reads
may contain multiple keys. In contrast, a non-streaming job's reducer
will have exactly one key value.

The reduce command reads stdin and writes to stdout. The stdin file
will look something like this:

\begin{verbatim}
aaa\t1
aaa\t2
bbb\t3
ccc\t4
ccc\t5
\end{verbatim}

Several reduce programs may be run. Each will receive all of the records
with a given key. Some may receive records with more than one key. You
must program accordingly. The output of the reduce function is a file in
the Hadoop file system. The file names have the form \code{part-NNNNN}.

The sample job \code{countInput} is a variant on the Unix program
\code{wc}. It reads a file in the Hadoop file system and produces a file
in the Hadoop file system that contains the number of records in the
input, the number of bytes in keys in the input, and the number of byte
in values in the input. Keys and values in the input file are separated
by tab characters. If there is no tab character in an input record, the
entire record is treated as a key.

The job requires two commands, which I put into two torch files:
\code{countInput-map.lua} and \code{countInput-reduce.lua}. Each file
must begin with a shebang line to invoke the torch interpreter. The
correct interpret to use is \code{torch-lua} as, when invoked in this
way, it does not print a greeting to stdout. If the greeting were
printed, it would appear in your output.

The map program \code{countInput-map.lua} reads records from stdin. It
attempts to break each record into a key and value. It counts the number
of records, the number of bytes in keys, and the number of bytes in
values.

The map and reduce programs share a function that splits an input record
into a key and value.

Here is the source code for the \code{countInput-map command}.

\verbatiminput{code/using-torch-on-hadoop/countInput-map.lua}


Here is the source code for the \code{countInput-reduce command}.

\verbatiminput{code/using-torch-on-hadoop/countInput-reduce.lua}


Here is the source code for the \code{getKeyValue} function.

\verbatiminput{code/using-torch-on-hadoop/getKeyValue.lua}

\subsection{Run your map-reduce job}

The job is run by the \code{countInput-run.sh} shell script below. It is
a bit complex and is described just after the listing. You run the
script by first changing to the directory where your lua source code is
and then running the command. In my setup, all the code is in one
directory: both the lua scripts and shell scripts, so I 

\begin{verbatim}
$ cd <source code directory>
$ ./countInput-run.sh
\end{verbatim}

You will need to make the shell script executable be executing the
command \code{\$ chmod +x countInput-run.sh}.

\verbatiminput{code/using-torch-on-hadoop/countInput-run.lua}

Near the top of the shell script, we set two variables to hold the name
of the input file and the name of the job. To make it easier to keep
track of things, we use these conventions.
\begin{itemize}
  \item The output of the map-reduce job is written to the Hadoop file
    system directory formed by concatenating the input file name, a dot,
    and the job name. The bash variable \code{OUTPUT\_DIR} is set
    accordingly.
  \item The name of the command for the mapper is formed by
    concatenating the job name with the string \code{-map.lua}.
    Likewise, the name of the reducer command is \code{-reduce.lua}.
    These names are in the bash variables \code{MAPPER} and
    \code{REDUCER}.
\end{itemize}

One quirk of the streaming interface is that it insists that the output
directory does not exist when the job is launched. Hence the directory
is deleted. Note that one cannot delete a non-existent directory in the
Hadoop file system, so that the script as written will fail the first
time it is run. The workaround is to comment out the \code{hadoop fs
-rmr} command on the initial run.

Next Hadoop is launched with the streaming interface. The option
\code{file} is set so that every \code{lua} script in the current
directory is copied to every execution node. Its necessary to copy only
the source code files actually used, but copying everything removes the
chance for an error and doesn't take much time.

\subsection{Copy output directory to local file system}

When the Hadoop streaming job finishes, the script copies its output
directory from the Hadoop file system to the local file system. Hadoop
will not copy to a local directory that already exists, so the local
directory is deleted if it exists. In the local file system, the output
directories are all subdirectories of \code{\$HOME/map-reduce-output}.
The copying is done because its easier
to work with files in the local file system than files in the Hadoop
file system.

The output directory will contain a \code{\_SUCCESS} or \code{\_FAILURE}
file that acts as a signal for the success of the map-reduce job. If the
job was successful, it will contain a code{part-NNNNN} file for each
reducer that ran.


\section{See also}

When you have many hadoop jobs to run, you will want to refactor this
solution to increase code usage. For a solution, see the recipe for ``I
want to run many Hadoop jobs.''

You will find that the debugging environment provided by Hadoop is
terrible compared to less complex and parallelized environments. We
provide some guidance on how to ease testing in the recipe ``I want to
test my Hadoop program.''

This recipe is free documentation. You can modify it by visiting the
github for account ``rlowrance'' and forking the repo
``torch-cookbook.''

\end{document}


