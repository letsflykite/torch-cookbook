% vim: set textwidth=72 :
% Copyright (C) 2013 Roy E Lowrance
% See the file COPYING.txt for copying conditions.

\documentclass{article}
\usepackage{amssymb,latexsym,amsmath}
\usepackage{listings}
\usepackage{verbatim}
\let\code\texttt % in line source code

\begin{document}
\title{Using Torch on Hadoop}
\author{Roy E Lowrance}
\maketitle

%\abstract{XXX}

\section{Problem}

You want to run a streaming job on NYU's Hadoop cluster using torch as
the programming language.

\section{Solution}

The solution is straight forward but requires getting a lot of details
rights.
\begin{enumerate}
  \item Get your data and torch scripts onto babar, the Hadoop cluster.
  \item Make sure a suitable instance of torch is available on babar,
    the Hadoop cluster.
  \item Break your map-reduce job into two torch programs. One will be
    the map command, the other the reduce command. A \emph{command} is a
    program that receives no command line options. 
  \item Use the streaming interface to submit a map-reduce job. The job
    will read a single file in the Hadoop file system. It will create a
    directory of files in the Hadoop file system.
  \item Copy the output directory from the Hadoop file system to the
    local file system for ease of examination and subsequent use.
\end{enumerate}

\section{Discussion}

We provide a worked example of a streaming job written in torch. The
code is short enough so that you can just type it in. It is also
available in the repository that holds this TeX code that generated this
document. How to get the source of this document is discussed in the
section ``See Also.''

\subsection{Move data and code to babar}

Before working through the example, there are some preliminary steps.
You need an NYU Hadoop logon. To get one, send a request in an email to
Yann LeCun. You need to install torch in your home directory 
on the Hadoop cluster. We provide a recipe for torch
installation in this cookbook.

Log in the the Hadoop cluster. In the example below, ID is your user
id. babar is the name of the login node for the Hadoop cluster.
(There is also \code{dumbo.es.its.nyu.edu}, but it doesn't have as much
software installed on it.)

\begin{verbatim}
$ ssh ID@hpc.nyu.edu
$ ssh babar.es.its.nyu.edu
\end{verbatim}


Now you should create a test file in the local file system. We provide
the file \code{courant-abel-prize-winners} which lists faculty of the
Courant Institute that won the Abel Prize. Each row contains the year of
the award, a tab character, and the name of the winner. Here's the file:

\verbatiminput{code/using-torch-on-hadoop/courant-able-prize-winners}

If you are using your own data and code, I recommend moving the data to
babar using \code{rsync} and your code to babar using \code{github}.

\subsection{Making a torch executable available}

You need an accessible instance of torch. For this you can either
install your own torch locally on babar in your user account or use
someone else's instance. I like to install my own so that I know the
torch I'm using on babar is exactly the same as the one on my
development system. To install torch locally, follow the recipe for
installing torch. At the time this note was written, Xiang
Zhang's torch install was available at \code{/home/xz558/.usr/bin}. Note
the dot before the \code{bin} directory. Xiang's torch install was up to
date when this note was written.

Be careful of the installed torch executables already on babar. At one time,
\code{dumbo}, an alternative Hadoop login node, had an installed version
of torch, but it was old and buggy.

To find out which torch executable you have, enter \code{\$which torch} in
a terminal. If you have installed torch and the \code{which} command
doesn't bring it up, fix your \code{\$PATH} variable by executing 
\code{EXPORT \$PATH:<directory with your torch>:\$PATH} in your
\code{.bash\_profile} script. If you edit your \code{.bash\_profile} you
will need to get bash to read it again. You do that by executing this
command in a terminal: \code{\$ source .bash\_profile}.

\subsection{Create the map-reduce commands}

Create two commands that will form your map-reduce job. Each command is
a torch source file that starts with a shebang. The source file is
invoked by the Hadoop run-time with no parameters, so it needs to be
self contained.

The map command reads stdin, which will be a file in the Hadoop file
system. It writes to stdout a series of records. Each record is
comprised of a key and a value separted by a tab character. Hadoop 
may run multiple instances of your map command. In fact, you hope that
it does, because that's where the speed up comes from. Each instance 
will produce a separate stdout key-value file.

Since you are using the streaming interface, you cannot specify a
combiner. 

When all the mappers finish running, their output files are split among
reducer nodes such that each reduce receives a file that has been sorted
by keys. Only one reducer is run for each key. The file a reducer reads
may contain multiple keys. In contrast, a non-streaming job's reducer
will have exactly one key value.

The reduce command reads stdin and writes to stdout. The stdin file
will look something like this:

\begin{verbatim}
aaa\t1
aaa\t2
bbb\t3
ccc\t4
ccc\t5
\end{verbatim}

Several reduce programs may be run. Each will receive all of the records
with a given key. Some may receive records with more than one key. You
must program accordingly. The output of the reduce function is a file in
the Hadoop file system. The file names have the form \code{part-NNNNN}.

The sample job \code{countInput} is a variant on the Unix program
\code{wc}. It reads a file in the Hadoop file system and produces a file
in the Hadoop file system that contains the number of records in the
input, the number of bytes in keys in the input, and the number of byte
in values in the input. Keys and values in the input file are separated
by tab characters. If there is no tab character in an input record, the
entire record is treated as a key.

The job requires two commands, which I put into two torch files:
\code{countInput-map.lua} and \code{countInput-reduce.lua}. Each file
must begin with a shebang line to invoke the torch interpreter. The
correct interpret to use is \code{torch-lua} as, when invoked in this
way, it does not print a greeting to stdout. If the greeting were
printed, it would appear in your output.

The map program \code{countInput-map.lua} reads records from stdin. It
attempts to break each record into a key and value. It counts the number
of records, the number of bytes in keys, and the number of bytes in
values.

The map and reduce programs share a function that splits an input record
into a key and value.

Here is the source code for the \code{countInput-map command}.

\verbatiminput{using-torch-on-hadoop-assets/countInput-map.lua}


Here is the source code for the \code{countInput-reduce command}.

\verbatiminput{using-torch-on-hadoop-assets/countInput-reduce.lua}


Here is the source code for the \code{getKeyValue} function.

\verbatiminput{using-torch-on-hadoop-assets/getKeyValue.lua}

\subsection{Run your map-reduce job}

The job is run by the \code{countInput-run.sh} shell script below. It is
a bit complex and is described just after the listing. I wrote it with
more comments than you may need, just in case you are a beginner. You
run the script by first changing to the directory where your lua source
code is and then running the command. In my setup, all the code is in
one directory: both the lua scripts and shell scripts, so I 

\begin{verbatim}
$ cd <source code directory>
$ ./countInput-run.sh
\end{verbatim}

You will need to make the shell script executable be executing the
command \code{\$ chmod +x countInput-run.sh}.

\verbatiminput{/using-torch-on-hadoop-assets/countInput-run.lua}

Here is a description of the shell script.
\begin{enumerate}
\item While debugging, I found these options useful.
\item Set up the name of the input file and the job name. The command
names are based on the job name.
\item Locate the input file and output directory.
\item Locate the hadoop streaming executable.
\item The input file must be in the Hadoop file system. If its not
there, copy it from the local file system.
\item The output directory cannot exist when the streaming job is
started. Delete it if it exists.
\item Run the streaming job. The lua source files need to be moved to
the Hadoop execution nodes my naming them in the \code{-file} option.
It's easiest to move all of the lua source code instead of just the
source code files used.
\item The streaming job write to a directory in the Hadoop file system.
It's easier to examine output in the local file system, so the script
copies the streaming output directory to the local file system. 
\item I'm eager to find out what happened, so I list the local output
directroy.
\item I know the result will be in a specific file in the output
directory, so I print that file.
\end{enumerate}


The output directory will contain a \code{\_SUCCESS} or \code{\_FAILURE}
file that acts as a signal for the success of the map-reduce job. If the
job was successful, it will contain a code{part-NNNNN} file for each
reducer that ran.


\section{See also}

When you have many hadoop jobs to run, you will want to refactor this
solution to increase code usage. For a solution, see the recipe for ``I
want to run many Hadoop jobs.''

You will find that the debugging environment provided by Hadoop is
terrible compared to less complex and parallelized environments. We
provide some guidance on how to ease testing in the recipe ``I want to
test my Hadoop program.''

This recipe is free documentation. You can modify it by visiting the
github for account ``rlowrance'' and forking the repo
``torch-cookbook.''

\end{document}


