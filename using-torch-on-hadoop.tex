% vim: set textwidth=72 :
% Copyright (C) 2013 Roy E Lowrance
% See the file COPYING.txt for copying conditions.

\documentclass{article}
\usepackage{amssymb,latexsym,amsmath}
\usepackage{listings}
\usepackage{verbatim}
\let\code\texttt % in line source code

\begin{document}
\title{Using Torch on Hadoop}
\author{Roy E Lowrance}
\maketitle

%\abstract{XXX}

\section{Problem}

You want to run a streaming job on NYU's Hadoop cluster using torch as
the programming language.

\section{Solution}

The solution is requires getting a lot of details right.\footnote{This
  solution could not have been developed without several discussion with
  Xiang Zhang, a Ph.D. student in computer science at New York
University.}

\begin{enumerate}
  \item Get your data and torch scripts onto babar, the Hadoop cluster.
  \item Make sure a suitable instance of torch is available on babar,
    the Hadoop cluster.
  \item Write two command that will form your map-reduce job. A
      \emph{command} is a string of characters that you could type and
      have the bash shell execute. A command is the name of a program to
      run followed by zero or more options. A lot of the art in using
      map-reduce is to decompose your job into these two commands.
  \item Use the streaming interface to submit a map-reduce job. The job
    will read a single file in the Hadoop file system. It will create a
    directory of files in the Hadoop file system.
  \item Copy the output directory from the Hadoop file system to the
    local file system for ease of examination and subsequent use.
\end{enumerate}

\section{Discussion}

We provide a worked example of a streaming job written in torch. The
code is short enough so that you can just type it in. It is also
available in the repository that holds this TeX code that generated this
document. How to get the source of this document is discussed in the
section ``See Also.''

\subsection{Move data and code to babar}

Before working through the example, there are some preliminary steps.
You need an NYU Hadoop logon. To get one, send a request in an email to
Yann LeCun. You need to install torch in your home directory 
on the Hadoop cluster. We provide a recipe for torch
installation in this cookbook.

Log into the the Hadoop cluster. In the example below, ID is your user
id. babar is the name of the login node for the Hadoop cluster.

\begin{verbatim}
$ ssh ID@hpc.nyu.edu
$ ssh babar 
\end{verbatim}


Now you should create a test file in the local file system. We provide
the file \code{courant-abel-prize-winners.txt} which lists faculty of the
Courant Institute who won the Abel Prize. Each row contains the year of
the award, a tab character, and the name of the winner. Here's the file:

\verbatiminput{using-torch-on-hadoop-assets/courant-abel-prize-winners.txt}

If you are using your own data and code, I recommend moving the data to
babar using \code{rsync} and your code to babar using \code{github} and 
\code{git}.

\subsection{Making a torch executable available}

You need an accessible instance of torch. For this you can either
install your own torch locally on babar in your user account or use
someone else's instance. I like to install my own so that I know the
torch I'm using on babar is exactly the same as the one on my
development system. To install torch locally, follow the recipe for
installing torch. At the time this note was written, Xiang
Zhang's torch install was available at \code{/home/xz558/.usr/bin}. Note
the dot before the \code{usr} directory. Xiang's torch install was up to
date when this note was written.

Be careful of the installed torch executables that may already
be on babar. They may have been built with different choices than the
torch you normally use and hence your torch program may run differently;
debugging streaming jobs on Hadoop is difficult enought with this
additional layer of complexity.


To find out which torch executable you have, enter \code{\$ which torch} in
a terminal. If you have installed torch and the \code{which} command
doesn't bring it up, fix your \code{\$PATH} variable by executing 
\code{EXPORT \$PATH=<directory with your torch>:\$PATH} in your
\code{.bash\_profile} script. If you edit your \code{.bash\_profile} you
will need to get bash to read it again. You do that by executing this
command in a terminal: \code{\$ source .bash\_profile}.

\subsection{Create the map-reduce commands}

Create two commands that will form your map-reduce job. Each command is
the name of a torch source file followed by zero or more options. The
options are read by the source file when it is executed. The source file
should start
with a shebang {an exclamation mark).

The map command reads stdin, which will be a file in the Hadoop file
system. It writes to stdout a series of records. Each record is
comprised of a key and a value separated by a tab character. Hadoop 
may run multiple instances of your map command. In fact, you hope that
it does, because that's where the speed up comes from. Each instance 
will read a unique portion of the input file and will
produce a separate stdout key-value file.

Since you are using the streaming interface, you cannot specify a
combiner. 

When all the mappers finish running, their output files are combined and
then split into one or more input files for the reducers to read. The
split is such that each input to a reducer contains all the output
records with a given key value.

The reduce command reads stdin and writes to stdout. The stdin file
will look something like this:

\begin{verbatim}
aaa\t1
aaa\t2
bbb\t3
ccc\t4
ccc\t5
\end{verbatim}

Several reduce programs may be run. Each will receive all of the records
with a given key. Some may receive records with more than one key. You
must program accordingly. The output of the reduce function is a file in
the Hadoop file system. The file names have the form \code{part-NNNNN}.

The sample job \code{countInput} is a variant on the Unix program
\code{wc}. It reads a file in the Hadoop file system and produces a
directory
in the Hadoop file system. 

The input file is a text file. Each record may be split into a key
portion followed by a value portion. If there is a tab character in the
record, all the characters before the first tab character form the key
and all the characters after it form the value.

If the map-reduce job is successful, the output directory contains
\code{part-NNNNN} files which contain the number of records in the
input, the number of bytes in keys in the input, and the number of byte
in values in the input. Keys and values in the input file are separated
by tab characters. There will be one output file for each reducer that
is run. The test data for the sample program is small enough so that
there is only one such file. If more data were used as input, there
would be multiple output files that would need to be combined.


The job requires two commands, which I put into two torch files:\\
\code{countInput-map.lua} and \code{countInput-reduce.lua}. Each file
must begin with a shebang line to invoke the torch interpreter. The
correct interpret to use is \code{torch-lua} as, when invoked in this
way, prints its greating messages to stderr, which is discarded by the
map-reduce framework. If the greeting were printed to stdout, it would
appear in your output file. (Note; It may be possible to invoke the
torch programs via a command such as \code{torch countInput-map.lua},
but I haven't tested this.)

The map program \code{countInput-map.lua} reads records from stdin. It
attempts to break each record into a key and value. It counts the number
of records, the number of bytes in keys, and the number of bytes in
values. In order to demonstrate that command options can be read, the
program checks for the presence of one option.

TODO: Determine if the torch library command to parse the command line
can be used. The capability of using the library is critical to
effective use of Hadoop.

The map and reduce programs share a function that splits an input record
into a key and value.

Here is the source code for \code{countInput-map.lua}.

\verbatiminput{using-torch-on-hadoop-assets/countInput-map.lua}

The reduce program \code{countInput-reduce.lua} reads key-value pairs
from stdin, This time the key is the name of the feature being counted
and the value is the number of times the was seen. The input file is
sorted by key, so the program keeps a running total and prints it when
the feature changes.

Here is the source code for \code{countInput-reduce.lua}.

\verbatiminput{using-torch-on-hadoop-assets/countInput-reduce.lua}

The shared function \code{getKeyValue} splits a string around the first
tab value in the string.

Here is the source code for the \code{getKeyValue.lua}.

\verbatiminput{using-torch-on-hadoop-assets/getKeyValue.lua}

\subsection{Run your map-reduce job}

TODO: Convert to a torch script instead of a shell script. Then students
don't have to learn the complicated shell language.

The job is run by the \code{countInput-run.sh} shell script below. It is
a bit complex and is described just after the listing. I wrote it with
more comments than you may need, just in case you are a beginner. You
run the script by first changing to the directory where your lua source
code is and then running the command. In my setup, all the code is in
one directory: both the lua scripts and shell scripts, so I 

\begin{verbatim}
$ cd <source code directory>
$ ./countInput-run.sh
\end{verbatim}

You will need to make the shell script executable be executing the
command \code{\$ chmod +x countInput-run.sh}. 

I use a few naming conventions in the shell script.
\begin{itemize}
  \item If the name of the job is \code{JOBNAME}, the name of the mapper
    is \code{JOBNAME-map.lau} and the name of the reducer is
    \code{JOBNAME-reduce.lau}.
  \item If the input file is \code{INPUT}, the output directory is
    \code{INPUT.JOBNAME}.
  \item The script copies the output directory the local directory\\
    \code{\$HOME/map-reduce-output/INPUT.JOBNAME}.
\end{itemize}

Here is the shell script \code{countInput-run.sh}. It is followed with a
description of each of the steps.

\verbatiminput{using-torch-on-hadoop-assets/countInput-run.sh}

Here is a description of the shell script.
\begin{enumerate}
\item While debugging, I found these options useful.
\item Set up the name of the input file and the job name. The command
names are based on the job name.
\item Locate the input file and output directory.
\item Locate the hadoop streaming executable.
\item The input file must be in the Hadoop file system. If its not
there, copy it from the local file system.
\item The output directory cannot exist when the streaming job is
started. Delete it if it exists.
\item Run the streaming job. The lua source files need to be moved to
the Hadoop execution nodes my naming them in the \code{-file} option.
It's easiest to move all of the lua source code instead of just the
source code files used.
\item The streaming job writes to a directory in the Hadoop file system.
It's easier to examine output in the local file system, so the script
copies the streaming output directory to the local file system. 
\item I'm eager to find out what happened, so I list the local output
directory.
\item I know the result will be in a specific file in the output
directory, so I print that file.
\end{enumerate}


The output directory will contain a \code{\_SUCCESS} or \code{\_FAILURE}
file that acts as a signal for the success of the map-reduce job. If the
job was successful, it will contain a \code{part-NNNNN} file for each
reducer that ran.


\section{See also}

When you have many hadoop jobs to run, you will want to refactor this
solution to increase code usage. For a solution, see the recipe for ``I
want to run many Hadoop jobs.''

You will find that the debugging environment provided by Hadoop is
terrible compared to less complex and parallelized environments. We
provide some guidance on how to ease testing in the recipe ``I want to
test my Hadoop program.''

This recipe is free documentation. You can modify it by visiting
github for account ``rlowrance'' and forking the repo
``torch-cookbook.''

\end{document}


